# Fine-tuning LLaMA 3.1 Using Unsloth for Legal Domain Specialisation

## ğŸ“„ Description

This repository documents my ongoing work in adapting Metaâ€™s LLaMA 3.1 8B model for domain-specific applications in Singaporean law. It builds upon and extends the excellent work from [`xuyangbocn/LLM-RAG-singapore-lawyer`](https://github.com/xuyangbocn/LLM-RAG-singapore-lawyer), which establishes a RAG pipeline for statute-based question answering.

My objectives include:

- Specialising the embedding model via LoRA fine-tuning using [Unsloth](https://github.com/unslothai/unsloth)
- Aligning the model outputs to legal standards through Reinforcement Learning with Human Feedback (RLHF) in collaboration with a professional lawyer
- Deploying an offline-ready LLM for legal document summarisation and retrieval

---

## ğŸ§  Project Scope and Roadmap

<details>
<summary><strong>ğŸš€ Project Phases</strong></summary>

### âœ… Phase 0: Baseline RAG Framework (Completed)

- âœ”ï¸ Investigated and adapted the `LLM-RAG-singapore-lawyer` architecture
- âœ”ï¸ Performed chunking and metadata tagging of legal statutes (PDFs)
- âœ”ï¸ Developed an initial RAG pipeline for legal QA using `LangChain`, `FAISS`, and `OpenChat`

---

### âš™ï¸ Phase 1: Domain-Specific Embedding Fine-tuning (In Progress)

- âŒ Prepare legal-specific contrastive data (e.g., semantically similar/dissimilar statute pairs)
- âŒ Fine-tune LLaMA 3.1 8B embeddings using LoRA via Unsloth
- âŒ Use quantised training (4-bit QLoRA) for efficient compute usage
- âŒ Evaluate domain specialisation using retrieval recall and legal topic clustering

---

### ğŸ§‘â€âš–ï¸ Phase 2: RLHF with Legal Expert

- âŒ Collect human preference data on model completions from a practising lawyer
- âŒ Train a reward model to reflect legal accuracy, helpfulness, and clarity
- âŒ Apply PPO or DPO for alignment with legal preferences
- âŒ Evaluate using legal benchmarks and human qualitative assessments

---

### ğŸ§ª Phase 3: Experimental Improvements (Planned)

- âŒ Investigate data curriculum learning for legal tasks (e.g., by statute complexity)
- âŒ Explore multi-turn dialogue alignment in legal consultations
- âŒ Test multi-objective RLHF with factuality and compliance scores

---

### ğŸ“¤ Phase 4: Deployment

- âŒ Convert and quantise the model (GGUF/ONNX) for offline inference
- âŒ Integrate with lightweight web UI (e.g., Gradio)
- âŒ Package as a containerised app with offline RAG + inference capabilities

</details>

---

## ğŸ›  Languages and Frameworks

- **Python**: >= 3.9 and <= 3.12  
- **Unsloth** for efficient LoRA fine-tuning  
- **LangChain**, **FAISS**, **HuggingFace Transformers**
- **PyTorch**, **PEFT**, **RLHF tooling (TRL/DPO)**

> Note: Unsloth installation on Windows is documented [here](https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation)

---

## ğŸ’» Development Environment

- **Operating System**: Windows 11 Pro  
- **Target Hardware**: Consumer-grade GPU (8-24 GB VRAM) or cloud-based A100s for RLHF

---

## ğŸ”— Credits and References

- [LLM-RAG-singapore-lawyer](https://github.com/xuyangbocn/LLM-RAG-singapore-lawyer)
- [Unsloth](https://github.com/unslothai/unsloth)
- [Hugging Face PEFT](https://github.com/huggingface/peft)
- [TRL (Transformers Reinforcement Learning)](https://github.com/huggingface/trl)

---

_This repository is maintained monthly and may not reflect the latest developments and updates. Please contact the author for the latest discussions. Feedback, suggestions, and collaborations are welcome._
