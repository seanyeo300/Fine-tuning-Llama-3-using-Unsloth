<h1>Fine-tuning-LLaMA-3-using-Unsloth</h1>

 <h2>Description</h2>
This repository documents my learning journey for fine-tuning LLaMA 3. Feel free to follow along using the curriculum provided.

<br />

## ğŸ§  Planned Curriculum: LLM Fine-tuning with LLaMA 3 + Unsloth

This section outlines my evolving curriculum for mastering LLM fine-tuning. It is structured progressively, from foundational concepts to advanced techniques, with a hands-on emphasis using the [Unsloth](https://github.com/unslothai/unsloth) framework and Meta's LLaMA 3 models.

---

### ğŸ“˜ Phase 1: Foundations of LLMs and Fine-tuning


### ğŸ“˜ Phase 1: Foundations of LLMs and Fine-tuning

- âœ”ï¸ Understand the Transformer architecture (Vaswani et al.)
- âŒ Review LLaMA 1/2/3 architectures (differences, improvements)
- âŒ Introduction to language modelling objectives (causal LM, MLM)
- âŒ Overview of fine-tuning vs instruction tuning vs RLHF
- âŒ Learn about parameter-efficient fine-tuning (PEFT) and LoRA

---

### ğŸ”§ Phase 2: Setting Up the Fine-tuning Stack

- âŒ Environment setup (CUDA, PyTorch, Transformers, Unsloth)
- âŒ Familiarise with Unsloth's LLaMA 3 API and model loading
- âŒ Tokenizer and data preprocessing pipeline
- âŒ Understand memory usage, model quantisation (4-bit/8-bit)

---

### ğŸ§ª Phase 3: Basic Fine-tuning

- âŒ Fine-tune LLaMA 3 using small datasets (e.g. Alpaca format)
- âŒ Use LoRA with Unsloth for efficient training
- âŒ Evaluate using perplexity and qualitative completions
- âŒ Save and resume training from checkpoints

---

### ğŸ§­ Phase 4: Instruction and Chat Fine-tuning

- âŒ Understand chat templates (e.g. `chatml`, `alpaca`, `zephyr`)
- âŒ Curate or format instruction-tuning datasets (e.g. ShareGPT, OpenHermes)
- âŒ Evaluate model responses using prompt-response format
- âŒ Apply techniques for avoiding mode collapse / overfitting

---

### ğŸ§  Phase 5: Advanced Topics

- âŒ Data curriculum learning (easy-to-hard instance ordering)
- âŒ Custom reward models and early stopping heuristics
- âŒ Multi-turn conversation alignment
- âŒ Multi-objective fine-tuning (e.g. factuality + helpfulness)

---

### âš™ï¸ Phase 6: Experiment Tracking and Optimisation

- âŒ Integrate with Weights & Biases or MLflow
- âŒ Profile memory and compute usage
- âŒ Hyperparameter tuning strategies (batch size, lr, scheduler)
- âŒ Ablation studies on LoRA ranks, dataset size, prompt formats

---

### ğŸ“¤ Phase 7: Deployment and Sharing

- âŒ Convert and quantise model for inference (GGUF, ONNX, etc.)
- âŒ Serve model using vLLM or Text Generation Inference
- âŒ Create Gradio/HF Spaces demo
- âŒ Publish LoRA adapters to Hugging Face Hub

---

_This roadmap is continuously evolving. Contributions, suggestions, and corrections are welcome._


<h2>Languages and Utilities Used</h2>

- <b>Python >=3.9; <=3.12  </b> 
- <b>Unsloth</b>

Installation instructions for unsloth on Windows can be found [here](https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation)

<h2>Environments Used </h2>

- <b>Windows 11</b> (Pro)



<!--
 ```diff
- text in red
+ text in green
! text in orange
# text in gray
@@ text in purple (and bold)@@
```
--!>
